name: Workflow to BigQuery

on:
  schedule:
    - cron: "*/30 * * * *"   # every 30 minutes
  workflow_dispatch:          # allow manual trigger

jobs:
  pipeline:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repo
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt google-cloud-bigquery google-cloud-storage elasticsearch requests aiohttp

    # Validate and write GCP key
    - name: Validate GCP key
      run: |
        echo "${{ secrets.GCP_SA_KEY }}" | tr -d '\r' | sed 's/\\n/\n/g' > ${{ github.workspace }}/gcp-key.json
        jq empty ${{ github.workspace }}/gcp-key.json
        jq -r .client_email ${{ github.workspace }}/gcp-key.json

    # Step 1: Fetch URLs from Elasticsearch
    - name: Run poller (fetch URLs)
      env:
        GOOGLE_APPLICATION_CREDENTIALS: ${{ github.workspace }}/gcp-key.json
        ES_ENDPOINT: "https://mi-reporting.es.us-west-2.aws.found.io"
        ES_API_KEY_ID: ${{ secrets.ES_API_KEY_ID }}
        ES_API_KEY_SECRET: ${{ secrets.ES_API_KEY_SECRET }}
      run: |
        python try_new_updates.py

    # Step 2: Download JSON reports
    - name: Download reports
      env:
        GOOGLE_APPLICATION_CREDENTIALS: ${{ github.workspace }}/gcp-key.json
      run: |
        python token_with_report.py

    # Step 3: Convert JSON → CSV
    - name: Process JSON to CSV
      run: |
        python updating_json_to_new_csv.py

    # Step 4: Append CSV → BigQuery
    - name: Upload to BigQuery
      env:
        GOOGLE_APPLICATION_CREDENTIALS: ${{ github.workspace }}/gcp-key.json
      run: |
        python upload_to_bigquery.py
